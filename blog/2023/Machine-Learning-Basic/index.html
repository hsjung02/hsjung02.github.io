<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Machine Learning Basic | Hyunseo Jung </title> <meta name="author" content="Hyunseo Jung"> <meta name="description" content="다양한 머신러닝 기법에 대하여"> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link defer href="/assets/css/bootstrap-toc.min.css?6f5af0bb9aab25d79b2448143cbeaa88" rel="stylesheet"> <link rel="shortcut icon" href="/assets/img/favicon.ico?41cf459f40947b93e393405eb0a22946"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://hsjung02.github.io/blog/2023/Machine-Learning-Basic/"> <script src="/assets/js/theme.js?a81d82887dd692e91686b43de4542f18"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Hyunseo</span> Jung </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="row"> <div class="col-sm-3"> <nav id="toc-sidebar" class="sticky-top"></nav> </div> <div class="col-sm-9"> <div class="post"> <header class="post-header"> <h1 class="post-title">Machine Learning Basic</h1> <p class="post-meta"> Created on June 21, 2023 </p> <p class="post-tags"> <a href="/blog/2023"> <i class="fa-solid fa-calendar fa-sm"></i> 2023 </a>   ·   <a href="/blog/tag/artificial-intelligence"> <i class="fa-solid fa-hashtag fa-sm"></i> Artificial Intelligence</a>   ·   <a href="/blog/category/notes"> <i class="fa-solid fa-tag fa-sm"></i> Notes</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <h1 id="linear-algebra">Linear algebra</h1> <h2 id="matrix-and-linear-transformation">Matrix and Linear transformation</h2> <p>Matrix는 사실 linear transformation이다.</p> <h2 id="projection">Projection</h2> <p>Projection은 \(P^2=P\)를 만족하는 linear transformation을 의미한다. Vector \(Y\)로의 projection은 \(P=\frac{YY^T}{Y^TY}\)로 나타낼 수 있다.</p> \[pf\rangle W=w\hat{Y}=w\frac{Y}{||Y||} \\w=||X||\cos\theta=||X||\frac{X\cdot Y}{||X||||Y||}=\frac{X\cdot Y}{||Y||}\\ W=w\hat{Y}=\frac{X\cdot Y}{||Y||}\frac{Y}{||Y||}=\frac{X^TY}{Y^TY}Y=\frac{\langle X,Y \rangle}{\langle Y,Y \rangle}Y=Y\frac{Y^TX}{Y^TY}=\frac{YY^T}{Y^TY}X=PX\] <h2 id="least-norm-solution">Least norm solution</h2> <p>Under-determined linear system \(AX=B\) 가 주어졌을 때, \(||X||^2\) 을 최소화하는 \(X^*=(A^TA)^{-1}A^TB\) 로 구할 수 있다.</p> \[pf\rangle A\perp(AX^*-B)\\A^T(AX^*-B)=0\\A^TAX^*=A^TB\\X^*=(A^TA)^{-1}A^TB\] <p>Proof using Lagrange multipliers</p> \[pf\rangle L(x,\lambda)=x^Tx+\lambda^T(Ax-y)\\\nabla_xL=2x+A^T\lambda=0, \nabla_\lambda L=Ax-y=0\\x=-\frac{A^T\lambda}{2}, \lambda=-2(AA^T)^{-1}y\\x=A^T(AA^T)^{-1}y\] <h1 id="optimization">Optimization</h1> <h2 id="optimization-1">Optimization</h2> <p>Optimization에는 3가지 요소가 존재한다.</p> <ol> <li>Objective function</li> <li>Decision variable</li> <li>Constraints</li> </ol> <p>Constraints를 만족하면서 objective function을 최대화/최소화하는 decision variable을 찾는 과정이 최적화이다.</p> <h2 id="convex-problem--convex-optimization">Convex problem &amp; Convex optimization</h2> <h3 id="convex-problem">Convex problem</h3> <p>Convex function: \(\forall x, y\in \mathbb{R}^n \ and \ \theta\in[0,1], f(\theta x+(1-\theta)y)\le\theta f(x)+(1-\theta)f(y)\)</p> <p>Convex set: \(\forall x, y \in C \ and \ \theta\in[0,1], \theta x+ (1-\theta)y\in C\)</p> <p>In convex problems, all local minimums are global minimums.</p> <h3 id="convex-optimization">Convex optimization</h3> <p>Any location where \(f'(x)=0\) is a flat point in the function, and a global minimum for convex problems.</p> <p>This argument also holds for multivariate function, and in this case we evaluate gradient of f, \(\nabla f\) instead of derivative.</p> <p>Then, how can we find the gradient of f? There are two solutions. Analytical solution and iterative solution.</p> <h2 id="matrix-derivatives">Matrix derivatives</h2> \[\nabla (Ax) = A^T\\ \nabla(x^TA)=A \\ \nabla(x^Tx)=2x \\ \nabla(x^TAx)=Ax+A^Tx\] <p>Examples</p> <ul> <li>Affine function \(g(x)=a^Tx+b\): \(\nabla g(x)=a, \nabla^2 g(x)=0\)</li> <li>Quadratic function \(g(x)=x^TPx+q^tX+r, P=P^T\): \(\nabla g(x)=2Px+q, \nabla^2 g(x)=2P\)</li> <li>L2 norm \(g(x)=||Ax-b||^2=x^TA^TAx-2b^TAx+b^Tb\): \(\nabla g(x)=2A^TAx-2A^Tb, \nabla^2 g(x)=2A^TA\)</li> </ul> <h2 id="iterative-method">Iterative method</h2> <p>In iterative method(or gradient descent), we iteratively update x as \(x \leftarrow x-\alpha\nabla_x f(x)\). Here, \(\alpha\) is step size. By iteratively updating x with negative gradient direction, we can reach to minimum.</p> <p>How to set step size? Step size should not be too small, but not too big as well. \(\alpha\) should satisfy: \(f(x_{i+1})-f(x_i)=\nabla f(x_i)\times(-\alpha_i \nabla f(x_i))\)</p> <h1 id="regression">Regression</h1> <h2 id="regression-1">Regression?</h2> <p>Finding the best function to describe given data (x, y) points.</p> <p>Regression is an optimization problem.</p> \[\min_\theta \sum_{i=1}^m(\hat{y}_i-y_i)^2 \\ s.t. \ \hat{y}_i=\theta x_i\] <h2 id="linear-regression">Linear regression</h2> <p>In linear regression, we find \(\theta_0, \theta_1\) where \(\hat{y}_i=\theta_0+\theta_1 x\) and \(\sum (\hat{y}_i-y_i)^2\) is minimized.</p> <p>There are two methods: linear algebra and gradient descent.</p> <h3 id="solve-using-linear-algebra">Solve using linear algebra</h3> <p>\(\theta=(A^TA)^{-1}A^Ty\) where \(A=\begin{pmatrix}1 &amp; x_1 \\\ 1 &amp; x_2 \\\ \vdots &amp; \vdots \\\ 1 &amp; x_n\end{pmatrix}\)</p> <h3 id="solve-using-gradient-descent">Solve using gradient descent</h3> \[f=(A\theta-y)^T(A\theta-y)=\theta^TA^TA\theta-\theta^TA^Ty-y^TA\theta+y^Ty\] \[\nabla f=A^TA\theta+A^TA\theta-A^Ty-A^Ty=2(A^TA\theta-A^Ty)\] \[\theta \leftarrow \theta-\alpha\nabla f\] <h3 id="multivariate-linear-regression">Multivariate linear regression</h3> <p>Formulation as follows:</p> \[\hat{y}_i=\theta_0+\theta_1x_1+\theta_2x_2\] \[\Phi=\begin{pmatrix}1 &amp; x_{1,1} &amp; x_{1,2}\\\ 1 &amp; x_{2,1} &amp; x_{2,2} \\\ \vdots &amp; \vdots &amp; \vdots \\\ 1 &amp; x_{m,1} &amp; x_{m,2}\end{pmatrix}\] \[\theta^*=(\Phi^T\Phi)^{-1}\Phi^Ty\] <h2 id="nonlinear-regression">Nonlinear regression</h2> <p>We can set nonlinear model. This is called nonlinear regression. However, nonlinear regression can be solved just the same as linear regression.</p> <p>There are two methods to solve nonlinear regression. Method 1 is to construct explicit feature vectors. Method 2 is to construct implicit feature vectors.</p> <h3 id="explicit-feature-vectors---polynomial-features">Explicit feature vectors - Polynomial features</h3> <p>Sup. model is \(y=\theta_0 + \theta_1x + \theta_2 x^2\). It looks like nonlinear, but it is a linear regression problem. We can set \(\Phi=\begin{pmatrix}1 &amp; x_1 &amp; x_1^2\\\ 1 &amp; x_2 &amp; x_2^2 \\\ \vdots &amp; \vdots &amp; \vdots \\\ 1 &amp; x_m &amp; x_m^2\end{pmatrix}\) and \(\hat{y}=\Phi\theta\). We can solve by \(\theta^*=(\Phi^T\Phi)^{-1}\Phi^Ty\)</p> <h3 id="explicit-feature-vectors---rbf-features">Explicit feature vectors - RBF features</h3> <p>RBF stands for Radial Basis Function. Define as: \(b_i(x)=\exp(-\frac{||x-\mu_i||^2}{2\sigma^2})\) and \(\hat{y}=\theta_0+\theta_1b_1(x)+\cdots+\theta_nb_n(x)\).</p> <h3 id="implicit-feature-vectors---kernel-trick">Implicit feature vectors - Kernel trick</h3> <p>Classification에서 자세히 다루겠다!</p> <h3 id="avoid-overfitting">Avoid overfitting</h3> <p>We can try three methods to avoid overfitting.</p> <p>First is to use less expensive features. For example, we can use lower degree polynomial, or we can use fewer RBF basis, or we can use larger RBF bandwidth.</p> <p>Second is to keep the magnitude of the parameter small. In many cases, overfitting is associated to large parameter value. This is implemented by adding \(||\theta||\) to constraint or to objective function. In this case, \(J=||\theta X-Y||^2+\lambda||\theta||^2\) and solution \(\theta^*=(X^TX+\lambda I)^{-1}X^Ty\).</p> <p>Third is to use L1 norm. Using L1 norm can be effective when outliers exist. since is more robust and insensitive to outliers. Using L1 norm is called LASSO(Least Absolute Shrinkage and Selection Operator). Using L2 norm is called ridge.</p> <h2 id="k-nearest-neighbor-regression">k-Nearest Neighbor regression</h2> <p>k-Nearest Neighbor (kNN) is a supervised and non-parametric method used for regression and classification. In kNN regression, when new point \(x_{new}\) is given, we make prediction \(y=avg(y|x\in N(x_{new}))\) where \(N(x_{new})\) is kNN of \(x_{new}\).</p> <h1 id="classification">Classification</h1> <h2 id="classification-1">Classification</h2> <p>In classification problem, output y is a discrete value. A classification model determines which class a new input should fall into.</p> <h2 id="perceptron">Perceptron</h2> <h3 id="concept">Concept</h3> <p>When linearly separable datas are given, we can use a perceptron. Perceptron describes a hyperplane which separates data into two classes. Hyperplane is defined by an outward pointing normal vector, \(\omega\), which is orthogonal to any vector on the hyperplane.</p> <h3 id="distance-related-to-perceptron">Distance related to perceptron</h3> <p>Consider a line \(g(x)=\omega_0+\omega^Tx=0\) . We can evaluate distance from a line for any vector \(x\). For any vector \(x\) , \(x=x_\perp+r\frac{\omega}{||\omega||}\) . Here \(r=d+h\) where \(d=-\frac{\omega_0}{||\omega||}\) is the distance from the origin to the line and \(h\) is the distance from the line to vector \(x\) . Then \(g(x)=\omega_0+\omega^Tx=\omega_0+r||\omega||=h||\omega||\) and \(h=\frac{g(x)}{||\omega||}\).</p> <h3 id="perceptron-algorithm">Perceptron Algorithm</h3> <p>As shown above, \(\omega\) is really important in perceptron. Then, how can we find \(\omega\) that well separates given data? We can apply the “Perceptron Algorithm”.</p> <aside> 💡 1. Randomly assign $$\omega$$ 2. One iteration of the PLA(Perceptron Learning Algorithm) $$\omega \leftarrow\omega +yx$$ where $$(x, y)$$ is a misclassified training point 3. At iteration $$i=1,2,3,\cdots,$$ pick a misclassified point from dataset 4. Run a PLA iteration on it </aside> <h2 id="non-separable-problem">Non-Separable problem</h2> <p>In real world, there are noise or outliers that makes a linearly non-separable case. In this case, we can apply three methods for classification. Those are using slack variables, SVM, and kernels.</p> <h2 id="relax-constraints-using-slack-variables">Relax constraints using slack variables</h2> <p>This method is used when there are some outliers. In this case, we allow outliers to be misclassified. However, we want to minimize the misclassified cases. Formulations are as follows:</p> \[\min \sum_{i=1}^Nu_i+\sum_{i=1}^Mv_i\\ s.t. \begin{cases}\omega^Tx^{(i)}\ge1-u_i &amp; (i=1,2,\cdots,N)\\\omega^Tx^{(N+i)}\le-(1-v_i) &amp; (i=1,2,\cdots,M)\\u\ge0\\v\ge0\end{cases}\] <h2 id="support-vector-machine">Support Vector Machine</h2> <p>We can improve above method using linear programming. The idea is that large margin(distance) leads to good generalization on the test data. In SVM, margin is defined as \(\mathrm{margin}=\frac{2}{||\omega||}\) (refer to the distance part). Maximizing margin means minimizing \(||\omega||\) , which is the closest samples from the decision line. We try to maximize minimum distance. Formulations are as follows:</p> \[\min ||\omega||+\gamma(u+v) \\s.t. \ \begin{cases}X_1\omega\ge1-u\\X_0\omega\le-(1-v)\\u\ge0\\v\ge0\end{cases}\] <h2 id="kernels">Kernels</h2> <p>If we do not want to allow misclassfications, or datas are non-linearly separable, we can use kernels. Kernel is a mapping of data to higher dimensions. For example, \(x=\begin{pmatrix}x_1 \\ x_2\end{pmatrix}\) can be expressed as \(z=\phi(x)=\begin{pmatrix}1 \\ x_1^2\\ x_1x_2\\ x_2^2\end{pmatrix}\).</p> <h2 id="logistic-regression">Logistic regression</h2> <p>In logistic regression, we consider distances from decision boundary to all data points. In SVM, we considered distances from decision boundary to two closest data points. Objective function to minimize is the multiplication of all distances. Here we use the technique of using sigmoid function for mapping, so the objective function becomes \(L(\omega)=\prod_{i=1}^nP(y^{(i)}|x^{(i);}\omega)=\prod_{i=1}^n(h_\omega(x^{(i)}))^{y^{(i)}}(1-h_\omega(x^{(i)}))^{1-y^{(i)}}\) . Using log likelihood, \(l(\omega)=\log L(\omega)=\sum_{i=1}^ny^{(i)}\log h_\omega(x^{(i)})+(1-y^{(i)})\log(1-h_\omega(x^{(i)}))\) and \(\hat{\omega}=\argmax_\omega l(\omega)\)</p> <h2 id="k-nearest-neighbor-classification">k-Nearest Neighbor classification</h2> <p>k-Nearest Neighbor (kNN) is a supervised and non-parametric method used for regression and classification. In kNN classification, when new point \(x_{new}\) is given, we make prediction \(y=maxlen(y|x\in N(x_{new}))\) where \(N(x_{new})\) is kNN of \(x_{new}\) .</p> <h1 id="k-means-clustering-algorithm">K-Means: Clustering algorithm</h1> <p>In clustering problem, it is important to catch similarities between samples. We need to make high within-cluster similarity and low inter-cluster similarity.</p> <p>K-Means is an itertive algorithm for clustering. K-Means algorithm is as follows:</p> <aside> ❔ init: randomly initialize cluster centeres do: 1. Cluster assignment for every points, find the nearest cluster center and assign the point to that cluster 2. Move centers move center to the mean of all points in the cluster for each clusters. </aside> <p>When using K-Means algorithm, we need to decide how many clusters we will use. For this, we need to gradually increase the number of clusters and find the elbow points.</p> <p>There are some limitations of k-means. First, it works well only for rounded shaped, equal size clusters. It works bad for non-convex clusters or clusters with different densities.</p> <h1 id="dimension-reduction">Dimension reduction</h1> <h2 id="principal-component-analysis">Principal Component Analysis</h2> <p>In PCA, we remove redundant features using highly correlated data. Let’s first talk about correlation.</p> <h3 id="correlation-and-covariance-matrix">Correlation and Covariance matrix</h3> <p>Variance tells how much the data is spread and covariance tells how two variables are spread. Easily speaking, covariance matrix</p> \[\sum=\begin{pmatrix}E(X_1-\mu_1)(X_1-\mu_1) &amp; E(X_1-\mu_1)(X_2-\mu_2) &amp; \cdots &amp; E(X_1-\mu_1)(X_n-\mu_n) \\\ \vdots &amp; \ddots &amp; \ddots &amp; \vdots \\\ E(X_n-\mu_n)(X_1-\mu_1) &amp; E(X_n-\mu_n)(X_2-\mu_2) &amp; \cdots &amp; E(X_n-\mu_n)(X_n-\mu_n)\end{pmatrix}\] <p>The goal of PCA is to find the direction that maximizes the variance and it can be done by doing eigenalanysis of covariance.</p> \[\begin{align*} (\mathrm{variance \ of \ projected \ data}) &amp;= \frac{1}{m}\sum_{i=1}^m(u^Tx_i)^2\\ &amp;=\frac{1}{m}\sum_{i=1}^m(x_i^Tu)^2\\ &amp;=\frac{1}{m}\sum_{i=1}^mu^Tx_ix_i^Tu\\ &amp;=u^T(\frac{1}{m}\sum_{i=1}^mx_ix_i^T)u\\ &amp;=u^TSu \end{align*}\] \[\max_u u^TSu\\ s.t.\ u^Tu=1\\ \to u^TSu=u^T\lambda u=\lambda\\ \therefore \max\lambda\] <p>PCA can be done by SVD of data matrix X: \(S=\frac{1}{m}X^TX=(\frac{X}{\sqrt{m}})^T(\frac{X}{\sqrt{m}})=A^TA=(U\sum V^T)^T(U\sum V^T)=V\Lambda V^T\). To find \(V\), just do SVD of \(X\).</p> <h2 id="fisher-discriminant-analysis">Fisher Discriminant Analysis</h2> <p>FDA is dimension reduction for labeled data. The goal is to maximize separation between classes while minimizing variance in each class.</p> \[\max_\omega\frac{(\mu_0^T\omega-\mu_1^T\omega)^2}{n_0\omega^TS_0\omega+n_1\omega^TS_1\omega}=\max_\omega\frac{(m^T\omega)^2}{\omega^T(n_0S_0+n_1S_1)\omega}\] <p>Let \(n_0S_0+n_1S_1=\sum=R^TR\) and \(u=R\omega\).</p> <p>\(J(u)=\frac{(m^TR^{-1}u)^2}{\omega^TR^TR\omega}=\frac{((R^{-T}m)^Tu)^2}{u^Tu}=((R^{-T}m)^T\frac{u}{||u||})^2\) is maximum when \(u\parallel R^{-T}m, \therefore u=\alpha R^{-T}m \to \omega=\alpha R^{-1}R^{-T}m=\alpha(n_0S_0+n_1S_1)^{-1}(\mu_0-\mu_1)\)</p> </div> </article> </div> </div> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Hyunseo Jung. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Last updated: April 12, 2025. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js?a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script defer src="/assets/js/bootstrap-toc.min.js?c82ff4de8b0955d6ff14f5b05eed7eb6"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/assets/js/search-setup.js?6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/assets/js/search-data.js"></script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>